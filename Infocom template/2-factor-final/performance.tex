\section{Evaluation}
\label{sec:Evaluation}

In this section, we evaluate the efficiency of the proposed two-step authentication framework. In this study, we mainly do two kinds of evaluations, one is the accuracy of the classifier, and the other is the distance measurement of sound wave transmission. Firstly, we present data collected by from different users and attackers. Furthermore, we measured the performances of each classifier that generated by different classification algorithm and present a detailed results for the efficiency of them. Finally, we present the performances of ultrasound transmission.

\subsection{Train Environment}
To test the effectiveness of our framework, we chose some mobile smart phones with rich sensors. We use Huawei Nova 2 plus as a login device to collect sensor data on the web and Vivo Y22L as the verification device to get sensor data by calling API on Android. The lists of sensors of Huawei Nova 2 plus and Vivo Y22L is given in Table \uppercase\expandafter{\romannumeral3}.




The Web-based sensor APIs mentioned before is an experimental technique, considering the compatibility of the device and the browser, we use the sensor mentioned in Table 2, without using sensors such as magnetic field, distance, temperature and so on, which may be applied in the future with the development of web technologies.

We collected data from multiple people in different regions. We divided the experimenters into two groups and corresponded one by one, one playing the "user" holding the verification device and the login device, and the other playing the "attacker" holding only the login device. The "user" sends the environmental states of the two devices as the $True$ data in different scenarios where the distance between the two devices owned by the "user" is within 9 meters. The "attacker" sends sensor data anywhere 9 meters away from the "user" and requests the verification device of the "user" to also send its sensor data. 


\begin{table*}

  \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}    
  
  \centering  
  \fontsize{6.5}{8}\selectfont  
  \begin{threeparttable}  
  \label{tab:table_distance}  
    \begin{tabular}{cccccccccccccc}  
    \toprule  
    \multirow{2}{*}{\bf Verification device}&  
    \multicolumn{4}{c}{ \bf{Volume-10\%}}&\multicolumn{4}{c}{ \bf{Volume-20\%}}&\multicolumn{4}{c}{ \bf{Volume-30\%}}\cr  
    \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}  
    &\tabincell{c}{\bf Huawei Nova \\\bf 2 plus}&\tabincell{c}{\bf Vivo \\\bf Y22L}&\tabincell{c}{\bf BLU \\\bf R1 HD}&\tabincell{c}{\bf Window\\\bf laptop}
    &\tabincell{c}{\bf Huawei Nova \\\bf 2 plus}&\tabincell{c}{\bf Vivo \\\bf Y22L}&\tabincell{c}{\bf BLU \\\bf R1 HD}&\tabincell{c}{\bf Window\\\bf laptop}
    &\tabincell{c}{\bf Huawei Nova \\\bf 2 plus}&\tabincell{c}{\bf Vivo \\\bf Y22L}&\tabincell{c}{\bf BLU \\\bf R1 HD}&\tabincell{c}{\bf Window\\\bf laptop} \cr
    \midrule
    \midrule  
    {\bf Huawei Nova 2 plus}& 
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$&
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$&
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$   
     \cr
    {\bf Vivo Y22L}& 
    10 $cm$& 13 $cm$& 15 $cm$& 1 $cm$& 
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$&
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$   
    \cr  
    {\bf BLU R1 HD}& 
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$&
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$&
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$   
      \cr  
    {\bf Window laptop}&
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$& 
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$&
    1 $cm$& 1 $cm$& 1 $cm$& 1 $cm$   
    \cr  
    \bottomrule  
    \end{tabular}  
    \end{threeparttable}  
    
    
    
  \caption{Distance measure of ultrasound transmission with different volume}  
\end{table*}



\subsection{Dataset}
To test our framework, we cross-validated raw data (datasets), which is very common \cite{Uluagac2013A}, with $75\%$ being the train set and the remaining $25\%$ being the validation set. First, we use the training set to train the classifier, and then use the verification set to test the trained model. The data were then used to train and test our framework for different machine learning algorithms.


\subsection{Performance metrics}
Since the first layer of out framework is fundamentally a classification problem, we have adopted four Performance Metrics:Accuracy, Precision, Recall and F1-score. True Positive (TP) indicates number of user' login that are detected correctly while true negative (TN) refers to the number of correctly detected login of attacker. On the other hand, False Positive (FP) states attacker's login that are detected as user's login and False Negative (FN) defines number of user's login that are categorized as attacker's login. F1-score is the harmonic mean of the accuracy rate and the recall rate. These performance metrics are defined as follows:

\begin{equation}\label{eq:12}
Accuracy=\frac{TN+TP}{TN+FP+FN+TP}
\end{equation}

\begin{equation}\label{eq:12}
Precision=\frac{TP}{TP+FP}
\end{equation}

\begin{equation}\label{eq:12}
Recall \ rate = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}\label{eq:12}
F1 \ score =\frac{2TP}{2TP+FP+FN}
\end{equation}




\subsection{Compression}
In this subsection, we present the performance of classifiers trained with different machine learning algorithms.
For all the approaches, we select the best possible case by judging parameters of training algorithm and report their performance metrics in Table \uppercase\expandafter{\romannumeral4}
These results indicate that xxx provides highest accuracy and F1-score compared to the other approaches.

\begin{table}

 \makeatletter\def\@captype{table}\makeatother

  \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}    
  
  \centering  
  \fontsize{6.5}{8}\selectfont  
  \begin{threeparttable}  
  \label{tab:table_ml}  
    \begin{tabular}{cccccccccccccc}  
    \toprule  
    \tabincell{c}{\bf Classification \\\bf algorithm}&{\bf Accuracy}&{\bf Precision}&{\bf Recall rate}&{\bf F1-score} \cr
    \midrule
    \midrule  
   SVM&0.8333&1.0000&0.7500&0.8571 \cr
Decision Tree&0.9167&1.0000&0.8571&0.9231 \cr
Logistic Regression&0.8333&1.0000&0.7500&0.8571 \cr
Random Forest&0.6667&1.0000&0.6000&0.7500 \cr
Kernal-SVM&0.8333&1.0000&0.7500&0.8571 \cr
Resilient Back-propagation&0.8333&0.8333&0.8333&0.8333 \cr
Back-Propagation&0.8333&0.8333&0.8333&0.8333 \cr
Levenberg-Marquardt&0.6667&1.0000&0.6000&0.7500 \cr

    \bottomrule  
    \end{tabular}  
    \end{threeparttable}  
    
    
  \caption{Comparison of different machine-learning based approaches proposed for first layer.}  
\end{table}



\subsection{Distance Measurement of sound wave transmission}

We mentioned two solutions to reduce the effective transmission distance of sound waves above. In the experiment, we found that the first solution works well. We built 16 sets of communication experiments with different brands and models of mobile phones, and used the currently commercialized sound wave transmission SDK to write a test app. We separately adjusted the volume of the verification phone to test the farthest distance that the login phone can receive and analyze the correct verification code. The experimental results are shown in Table \uppercase\expandafter{\romannumeral5}.

This table show that xxx provides highest accuracy and F1-score compared to the other approaches.

Due to limited time, we will complete the experiment of the second scheme in the future work, which not only can control the distance, but also calculate the distance between the verification device and the landing device in theory.


\iffalse
\subsection{Performance}
\label{sec:performance}

In order to study the feasibility of the four attacks, we implement them and evaluate them in
two aspects, \emph{time cost} and \emph{memory usage}.
% We check the average running time and the average virtual memory curves to illustrate the performance of our four proof-of-concept attacks.

% \subsubsection{Getting Virtual Memories of Third-party Apps}
% 1. No permissions required
% 2. Using the library \texttt{libsuperuser}
% 3. Using \texttt{toolbox}
% 4. Linux command "ps" which reports a snapshot of the current processes

\subsubsection{Time Cost}
We conduct 6 - 8 rounds of tests for every attack to calculate the average
running time cost. The testing results are summarized
in Table~\ref{tbl:time}. As shown in Table~\ref{tbl:time}, our attacks
are efficient since the time cost of most attacks is less than 8 seconds:
the UI Phishing attack costs 7.1 seconds, which includes user
interactions such as launching Instagram, entering username and
passwords; The Activity-in-the-middle attack costs time 5.8 seconds; and
the average time cost of the gallery-stealing attack is 4.6 seconds.

The Screenshot Capturing attack is an exception, taking 38.1
seconds. This attack uses much longer time because screenshots in
Android are passed in the format of RGBA, while we need to change to
ARGB in order to convert it to common format such as JPEG and PNG.
We perform the matrix transformation required for the conversion on
the mobile device.
%% We stress the matrices transformations to be run in the local app for
%% convenience, which takes much more time than other attacks.
% and thats why the time taken for the attack is much longer
% than other apps.
But since the process is undertaken in the background, it will
not trigger the suspicions of the user. In the real world attack, an
adversary can leave the job of matrices transformation to the server.

\begin{table}[h]
  \centering
\caption{Time Cost of Proof-of-Concept Attacks}
\label{tbl:time}
\begin{footnotesize}
\begin{tabular}{|c|c|}
  \hline
  \textbf{Attack} & \textbf{Time Cost (s)} \\
  \hline\hline
  UI Phishing & 7.1\\
  \hline
  Man-in-the-Middle & 5.8\\
  \hline
  Gallery Stealing & 4.6 \\
  \hline
  Screenshot Capturing & 38.1\\
  \hline
\end{tabular}
\end{footnotesize}
\end{table}

\subsubsection{Memory Usage}

We conduct 3-round experiments for each attacks to get the memory
distractions of attacks and evaluate their average memory usage. The
testing results are illustrated in Figure~\ref{fig:memory}.  It shows
that the maximum memory usage of most attacks is less than $70MB$ except
Gallery Stealing attack, whose memory usage is around $100MB$. Our
testing results also disclose the memory usage distribution on
different period, in which the major memory usage of each attack is to
launch victim app (normal app), e.g., Instagram and Facebook. The
memory usage differences caused by stealthy behaviors/operations are
negligible.

\begin{figure}[h]
        \centering
        \begin{subfigure}[t]{0.25\textwidth}
                \centering
               \includegraphics[width=0.975\linewidth]{attack1memory.pdf}
                \caption{UI Phishing}
        \end{subfigure}%
        %\quad
        \begin{subfigure}[t]{0.25\textwidth}
                \centering
          \includegraphics[width=0.975\linewidth]{attack2memory.pdf}
                \caption{Activity-in-the-middle}
        \end{subfigure}
        \begin{subfigure}[t]{0.25\textwidth}
                \centering
          \includegraphics[width=0.975\linewidth]{attack4memory.pdf}
                \caption{Gallery Stealing}
        \end{subfigure}%
        %\quad
        \begin{subfigure}[t]{0.25\textwidth}
                \centering
          \includegraphics[width=0.975\linewidth]{attack3memory.pdf}
                \caption{Screenshot Capturing}
        \end{subfigure}
         \caption{Memory Distribution Curves}
         \label{fig:memory}
\end{figure}

In addition, we also conduct experiment to monitor the battery consuming
status and evaluate the battery usage of our attacks. The results show
that our attacks may not cause influence on battery aspect (the
battery usage rates of most attacks are $0\%$).  Above all, the
experiment results demonstrate that our proof-of-concept attacks are
light-weight with limited permission requirements.
% %%insert android device photos
% \begin{figure*}[t!]
%         \centering
%         \begin{subfigure}[t]{0.5\textwidth}
%                 \centering

%                 \includegraphics[width=0.95\linewidth]{attack1memory}
%                 \caption{UI Phishing Memory Distribution}

%         \end{subfigure}%
%         %\quad
%         \begin{subfigure}[t]{0.5\textwidth}
%                 \centering
%           \includegraphics[width=0.95\linewidth]{attack2memory}

%                 \caption{MITM Memory Distribution}
%         \end{subfigure}
%          \begin{subfigure}[t]{0.5\textwidth}
%                 \centering
%           \includegraphics[width=0.95\linewidth]{attack4memory}

%                 \caption{Gallery Stealing Memory Distribution}
%         \end{subfigure}%
%    %\quad
%         \begin{subfigure}[t]{0.5\textwidth}
%                 \centering
%           \includegraphics[width=0.95\linewidth]{attack3memory}

%                 \caption{Screenshot Memory Distribution}
%         \end{subfigure}
%          \caption{Memory Curves}
% \end{figure*}

%%%%%figure ends



% \begin{table}[h]
% \caption{Time Cost of Proof-of-Concept Attacks}
% \label{tbl:time}
% \begin{footnotesize}
% \begin{tabular}{|c|c|c|c|c|}
%   \hline
% %  \multirow{2}{*}{\minitab[l]{Function\&\\Element}} & \multirow{2}{*}{$\cdots$} &
% %  \multirow{2}{*}{0.5-1.0s} & \multirow{2}{*}{0.0-0.5s} \\
% %   & & & \\
%   \textbf{Function/Element} & \textbf{$\cdots$} & \textbf{1.0-1.5s} & \textbf{0.5-1.0s} & \textbf{0.0-0.5s} \\
%   \hline
%   \scriptsize{document.getElementById} & $\cdots$ & 0 & 1 & 0 \\
%   \hline
%   window.setTimeout & $\cdots$ & 45 & 44 & 44 \\
%   \hline
%   Date.prototype.getTime & $\cdots$ & 263 & 262 & 263 \\
%   \hline
%   $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
%   \hline
%   No.122 \bracket{div} & $\cdots$ & 44 & 40 & 42 \\
%   \hline
%   No.125 \bracket{div} & $\cdots$ & 44 & 40 & 44 \\
%   \hline
%   $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
%   \hline
% \end{tabular}
% \end{footnotesize}
% \end{table}

\fi


